# Optuna Hyperparameter Tuning Configuration Template
# This template shows how to configure hyperparameter tuning using Optuna

max_samples: 1000  # Maximum number of training samples per trial
compute_objective_every_n_samples: 100  # How often to evaluate the objective metric
n_trials: 10  # Number of hyperparameter trials to run

network_params:
  # Example: Variable list parameter (e.g., number and size of layers)
  layers:
    mode: variable_list
    length:
      mode: int
      params:
        low: 1
        high: 3
    values:
      mode: int
      params:
        low: 10
        high: 100
  
  # Example: Simple integer parameter
  dropout_rate:
    mode: float
    params:
      low: 0.0
      high: 0.5

optimizer_params:
  method:
    mode: categorical
    params:
      choices: ["Adam", "SGD", "RMSprop"]
  lr:
    mode: float
    params:
      low: 0.0001
      high: 0.1
      log: true  # Use log scale for learning rate

loss_params:
  loss_fn:
    mode: categorical
    params:
      choices: ["CrossEntropyLoss", "MSELoss", "BCEWithLogitsLoss"]

data_params:
  batch_size:
    mode: categorical
    params:
      choices: [16, 32, 64, 128, 256]

# Optuna pruner configuration
pruner:
  name: MedianPruner
  params:
    n_warmup_steps: 10
    n_startup_trials: 5

# Optuna sampler configuration  
sampler:
  name: TPESampler
  params:
    seed: 42

# Optimization objective
objective:
  metric: "val_loss"  # Metric to optimize (should match model output)
  direction: minimize  # "minimize" or "maximize"
  


